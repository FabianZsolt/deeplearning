{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network, experimentation tool, version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDIT: the scaling is now done separately for independent support variables and the target. The model works a bit better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative values\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# we also need a derivated version of ReLu\n",
    "# otherwise same as original, but instead of original value, return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness in order to get same results everytime\n",
    "# you can change or disable this if you want\n",
    "np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 20)\n",
    "        n2 = np.random.randint(0, 50)\n",
    "\n",
    "\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 10)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 2, 173],\n",
       " [6, 17, 56],\n",
       " [10, 22, 123],\n",
       " [0, 49, 58],\n",
       " [0, 46, 46],\n",
       " [15, 25, 253],\n",
       " [14, 36, 232],\n",
       " [16, 4, 261],\n",
       " [3, 2, 15],\n",
       " [7, 2, 55],\n",
       " [15, 48, 280],\n",
       " [9, 35, 120],\n",
       " [6, 33, 74],\n",
       " [2, 33, 45],\n",
       " [3, 42, 56],\n",
       " [0, 11, 13],\n",
       " [10, 22, 124],\n",
       " [4, 15, 35],\n",
       " [6, 45, 84],\n",
       " [16, 6, 266],\n",
       " [7, 11, 66],\n",
       " [7, 1, 55],\n",
       " [18, 20, 352],\n",
       " [17, 12, 303],\n",
       " [17, 1, 293],\n",
       " [12, 41, 185],\n",
       " [17, 22, 314],\n",
       " [3, 11, 25],\n",
       " [7, 41, 92],\n",
       " [3, 11, 23],\n",
       " [19, 30, 397],\n",
       " [9, 23, 110],\n",
       " [19, 6, 373],\n",
       " [6, 17, 56],\n",
       " [3, 1, 10],\n",
       " [5, 40, 71],\n",
       " [14, 15, 219],\n",
       " [13, 49, 218],\n",
       " [3, 29, 39],\n",
       " [19, 4, 372],\n",
       " [6, 1, 41],\n",
       " [12, 3, 150],\n",
       " [7, 38, 95],\n",
       " [6, 13, 53],\n",
       " [4, 48, 64],\n",
       " [8, 24, 92],\n",
       " [13, 13, 190],\n",
       " [14, 6, 203],\n",
       " [6, 40, 83],\n",
       " [10, 12, 121],\n",
       " [10, 49, 156],\n",
       " [13, 44, 214],\n",
       " [9, 14, 103],\n",
       " [1, 3, 5],\n",
       " [14, 23, 224],\n",
       " [1, 37, 40],\n",
       " [18, 46, 379],\n",
       " [3, 18, 33],\n",
       " [9, 17, 101],\n",
       " [11, 26, 155],\n",
       " [11, 10, 134],\n",
       " [13, 23, 201],\n",
       " [14, 3, 202],\n",
       " [14, 21, 223],\n",
       " [16, 23, 286],\n",
       " [4, 4, 25],\n",
       " [16, 9, 267],\n",
       " [5, 33, 63],\n",
       " [9, 15, 98],\n",
       " [10, 36, 139],\n",
       " [16, 3, 266],\n",
       " [2, 37, 42],\n",
       " [17, 2, 299],\n",
       " [0, 25, 28],\n",
       " [3, 1, 17],\n",
       " [6, 35, 72],\n",
       " [7, 18, 73],\n",
       " [5, 9, 42],\n",
       " [9, 4, 92],\n",
       " [8, 21, 94],\n",
       " [18, 36, 363],\n",
       " [19, 17, 385],\n",
       " [12, 40, 189],\n",
       " [14, 14, 217],\n",
       " [16, 48, 312],\n",
       " [15, 49, 274],\n",
       " [14, 46, 246],\n",
       " [11, 47, 173],\n",
       " [4, 21, 37],\n",
       " [7, 1, 52],\n",
       " [10, 18, 125],\n",
       " [7, 35, 90],\n",
       " [1, 40, 45],\n",
       " [17, 45, 340],\n",
       " [10, 47, 147],\n",
       " [3, 49, 61],\n",
       " [7, 39, 95],\n",
       " [19, 14, 378],\n",
       " [5, 1, 31],\n",
       " [9, 17, 101]]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bmi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "charges",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ed2b714d-eab8-4868-bebd-5c57b9d2d1a1",
       "rows": [
        [
         "0",
         "19",
         "27.9",
         "16884.924"
        ],
        [
         "1",
         "18",
         "33.77",
         "1725.5523"
        ],
        [
         "2",
         "28",
         "33.0",
         "4449.462"
        ],
        [
         "3",
         "33",
         "22.705",
         "21984.47061"
        ],
        [
         "4",
         "32",
         "28.88",
         "3866.8552"
        ],
        [
         "5",
         "31",
         "25.74",
         "3756.6216"
        ],
        [
         "6",
         "46",
         "33.44",
         "8240.5896"
        ],
        [
         "7",
         "37",
         "27.74",
         "7281.5056"
        ],
        [
         "8",
         "37",
         "29.83",
         "6406.4107"
        ],
        [
         "9",
         "60",
         "25.84",
         "28923.13692"
        ],
        [
         "10",
         "25",
         "26.22",
         "2721.3208"
        ],
        [
         "11",
         "62",
         "26.29",
         "27808.7251"
        ],
        [
         "12",
         "23",
         "34.4",
         "1826.843"
        ],
        [
         "13",
         "56",
         "39.82",
         "11090.7178"
        ],
        [
         "14",
         "27",
         "42.13",
         "39611.7577"
        ],
        [
         "15",
         "19",
         "24.6",
         "1837.237"
        ],
        [
         "16",
         "52",
         "30.78",
         "10797.3362"
        ],
        [
         "17",
         "23",
         "23.845",
         "2395.17155"
        ],
        [
         "18",
         "56",
         "40.3",
         "10602.385"
        ],
        [
         "19",
         "30",
         "35.3",
         "36837.467"
        ],
        [
         "20",
         "60",
         "36.005",
         "13228.84695"
        ],
        [
         "21",
         "30",
         "32.4",
         "4149.736"
        ],
        [
         "22",
         "18",
         "34.1",
         "1137.011"
        ],
        [
         "23",
         "34",
         "31.92",
         "37701.8768"
        ],
        [
         "24",
         "37",
         "28.025",
         "6203.90175"
        ],
        [
         "25",
         "59",
         "27.72",
         "14001.1338"
        ],
        [
         "26",
         "63",
         "23.085",
         "14451.83515"
        ],
        [
         "27",
         "55",
         "32.775",
         "12268.63225"
        ],
        [
         "28",
         "23",
         "17.385",
         "2775.19215"
        ],
        [
         "29",
         "31",
         "36.3",
         "38711.0"
        ],
        [
         "30",
         "22",
         "35.6",
         "35585.576"
        ],
        [
         "31",
         "18",
         "26.315",
         "2198.18985"
        ],
        [
         "32",
         "19",
         "28.6",
         "4687.797"
        ],
        [
         "33",
         "63",
         "28.31",
         "13770.0979"
        ],
        [
         "34",
         "28",
         "36.4",
         "51194.55914"
        ],
        [
         "35",
         "19",
         "20.425",
         "1625.43375"
        ],
        [
         "36",
         "62",
         "32.965",
         "15612.19335"
        ],
        [
         "37",
         "26",
         "20.8",
         "2302.3"
        ],
        [
         "38",
         "35",
         "36.67",
         "39774.2763"
        ],
        [
         "39",
         "60",
         "39.9",
         "48173.361"
        ],
        [
         "40",
         "24",
         "26.6",
         "3046.062"
        ],
        [
         "41",
         "31",
         "36.63",
         "4949.7587"
        ],
        [
         "42",
         "41",
         "21.78",
         "6272.4772"
        ],
        [
         "43",
         "37",
         "30.8",
         "6313.759"
        ],
        [
         "44",
         "38",
         "37.05",
         "6079.6715"
        ],
        [
         "45",
         "55",
         "37.3",
         "20630.28351"
        ],
        [
         "46",
         "18",
         "38.665",
         "3393.35635"
        ],
        [
         "47",
         "28",
         "34.77",
         "3556.9223"
        ],
        [
         "48",
         "60",
         "24.53",
         "12629.8967"
        ],
        [
         "49",
         "36",
         "35.2",
         "38709.176"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2772
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use generated training data from our helper function\n",
    "# data = generate_train_data()\n",
    "# df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])\n",
    "df = pd.read_csv(\"medical_insurance.csv\")\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]\n",
    "#df = df[:500]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 0.046618627060119065\n",
      "Epoch: 2, loss: 0.03846307578944744\n",
      "Epoch: 3, loss: 0.037983465008530476\n",
      "Epoch: 4, loss: 0.03786745154886213\n",
      "Epoch: 5, loss: 0.03782132243088398\n",
      "Epoch: 6, loss: 0.03780183036154379\n",
      "Epoch: 7, loss: 0.03779497378699052\n",
      "Epoch: 8, loss: 0.03779045949247771\n",
      "Epoch: 9, loss: 0.037786117623761\n",
      "Epoch: 10, loss: 0.03778335625206463\n",
      "Epoch: 11, loss: 0.03778079994076388\n",
      "Epoch: 12, loss: 0.0377780861542083\n",
      "Epoch: 13, loss: 0.03777537028037886\n",
      "Epoch: 5, loss: 0.03782132243088398\n",
      "Epoch: 6, loss: 0.03780183036154379\n",
      "Epoch: 7, loss: 0.03779497378699052\n",
      "Epoch: 8, loss: 0.03779045949247771\n",
      "Epoch: 9, loss: 0.037786117623761\n",
      "Epoch: 10, loss: 0.03778335625206463\n",
      "Epoch: 11, loss: 0.03778079994076388\n",
      "Epoch: 12, loss: 0.0377780861542083\n",
      "Epoch: 13, loss: 0.03777537028037886\n",
      "Epoch: 14, loss: 0.03777280084415057\n",
      "Epoch: 15, loss: 0.037770302489461546\n",
      "Epoch: 16, loss: 0.03776802808158401\n",
      "Epoch: 17, loss: 0.037766764418037835\n",
      "Epoch: 18, loss: 0.037765677388059894\n",
      "Epoch: 19, loss: 0.03776462414269057\n",
      "Epoch: 20, loss: 0.03776363652493612\n",
      "Epoch: 14, loss: 0.03777280084415057\n",
      "Epoch: 15, loss: 0.037770302489461546\n",
      "Epoch: 16, loss: 0.03776802808158401\n",
      "Epoch: 17, loss: 0.037766764418037835\n",
      "Epoch: 18, loss: 0.037765677388059894\n",
      "Epoch: 19, loss: 0.03776462414269057\n",
      "Epoch: 20, loss: 0.03776363652493612\n",
      "Epoch: 21, loss: 0.037762729707234266\n",
      "Epoch: 22, loss: 0.037762288464665665\n",
      "Epoch: 23, loss: 0.03776193873598714\n",
      "Epoch: 24, loss: 0.037761587327286884\n",
      "Epoch: 25, loss: 0.0377612396017666\n",
      "Epoch: 26, loss: 0.03776089624456238\n",
      "Epoch: 27, loss: 0.037760557537994614\n",
      "Epoch: 28, loss: 0.037760223615290495\n",
      "Epoch: 29, loss: 0.0377598945151285\n",
      "Epoch: 30, loss: 0.03775967204131454\n",
      "Epoch: 21, loss: 0.037762729707234266\n",
      "Epoch: 22, loss: 0.037762288464665665\n",
      "Epoch: 23, loss: 0.03776193873598714\n",
      "Epoch: 24, loss: 0.037761587327286884\n",
      "Epoch: 25, loss: 0.0377612396017666\n",
      "Epoch: 26, loss: 0.03776089624456238\n",
      "Epoch: 27, loss: 0.037760557537994614\n",
      "Epoch: 28, loss: 0.037760223615290495\n",
      "Epoch: 29, loss: 0.0377598945151285\n",
      "Epoch: 30, loss: 0.03775967204131454\n",
      "Epoch: 31, loss: 0.03775957022999\n",
      "Epoch: 32, loss: 0.03775946867867553\n",
      "Epoch: 33, loss: 0.037759367519019774\n",
      "Epoch: 34, loss: 0.03775926683461136\n",
      "Epoch: 35, loss: 0.03775916667788234\n",
      "Epoch: 36, loss: 0.03775906708088387\n",
      "Epoch: 31, loss: 0.03775957022999\n",
      "Epoch: 32, loss: 0.03775946867867553\n",
      "Epoch: 33, loss: 0.037759367519019774\n",
      "Epoch: 34, loss: 0.03775926683461136\n",
      "Epoch: 35, loss: 0.03775916667788234\n",
      "Epoch: 36, loss: 0.03775906708088387\n",
      "Epoch: 37, loss: 0.03775896806233184\n",
      "Epoch: 38, loss: 0.03775886963221892\n",
      "Epoch: 39, loss: 0.03775877179483665\n",
      "Epoch: 40, loss: 0.037758674550757466\n",
      "Epoch: 41, loss: 0.03775857789813562\n",
      "Epoch: 42, loss: 0.037758481833562355\n",
      "Epoch: 37, loss: 0.03775896806233184\n",
      "Epoch: 38, loss: 0.03775886963221892\n",
      "Epoch: 39, loss: 0.03775877179483665\n",
      "Epoch: 40, loss: 0.037758674550757466\n",
      "Epoch: 41, loss: 0.03775857789813562\n",
      "Epoch: 42, loss: 0.037758481833562355\n",
      "Epoch: 43, loss: 0.03775838635262802\n",
      "Epoch: 44, loss: 0.03775829145029178\n",
      "Epoch: 45, loss: 0.03775819712112522\n",
      "Epoch: 46, loss: 0.037758103359472404\n",
      "Epoch: 47, loss: 0.037758010159555606\n",
      "Epoch: 48, loss: 0.037757917515543894\n",
      "Epoch: 49, loss: 0.03775782542159974\n",
      "Epoch: 50, loss: 0.037757733871907506\n",
      "Epoch: 43, loss: 0.03775838635262802\n",
      "Epoch: 44, loss: 0.03775829145029178\n",
      "Epoch: 45, loss: 0.03775819712112522\n",
      "Epoch: 46, loss: 0.037758103359472404\n",
      "Epoch: 47, loss: 0.037758010159555606\n",
      "Epoch: 48, loss: 0.037757917515543894\n",
      "Epoch: 49, loss: 0.03775782542159974\n",
      "Epoch: 50, loss: 0.037757733871907506\n",
      "Epoch: 51, loss: 0.03775764286069334\n",
      "Epoch: 52, loss: 0.03775755238223755\n",
      "Epoch: 53, loss: 0.0377574624308822\n",
      "Epoch: 54, loss: 0.037757373001035835\n",
      "Epoch: 55, loss: 0.03775728408717656\n",
      "Epoch: 56, loss: 0.037757195683853266\n",
      "Epoch: 57, loss: 0.03775710778568624\n",
      "Epoch: 58, loss: 0.03775702067147908\n",
      "Epoch: 51, loss: 0.03775764286069334\n",
      "Epoch: 52, loss: 0.03775755238223755\n",
      "Epoch: 53, loss: 0.0377574624308822\n",
      "Epoch: 54, loss: 0.037757373001035835\n",
      "Epoch: 55, loss: 0.03775728408717656\n",
      "Epoch: 56, loss: 0.037757195683853266\n",
      "Epoch: 57, loss: 0.03775710778568624\n",
      "Epoch: 58, loss: 0.03775702067147908\n",
      "Epoch: 59, loss: 0.03775693521651469\n",
      "Epoch: 60, loss: 0.03775685026453029\n",
      "Epoch: 61, loss: 0.03775676577682835\n",
      "Epoch: 62, loss: 0.0377566817107066\n",
      "Epoch: 63, loss: 0.03775659810300201\n",
      "Epoch: 64, loss: 0.037756514940814084\n",
      "Epoch: 65, loss: 0.03775643224426761\n",
      "Epoch: 59, loss: 0.03775693521651469\n",
      "Epoch: 60, loss: 0.03775685026453029\n",
      "Epoch: 61, loss: 0.03775676577682835\n",
      "Epoch: 62, loss: 0.0377566817107066\n",
      "Epoch: 63, loss: 0.03775659810300201\n",
      "Epoch: 64, loss: 0.037756514940814084\n",
      "Epoch: 65, loss: 0.03775643224426761\n",
      "Epoch: 66, loss: 0.03775634997966129\n",
      "Epoch: 67, loss: 0.03775626813065524\n",
      "Epoch: 68, loss: 0.0377561867130019\n",
      "Epoch: 69, loss: 0.03775610573757853\n",
      "Epoch: 70, loss: 0.03775602515703899\n",
      "Epoch: 71, loss: 0.037755945006395314\n",
      "Epoch: 72, loss: 0.037755865270908295\n",
      "Epoch: 66, loss: 0.03775634997966129\n",
      "Epoch: 67, loss: 0.03775626813065524\n",
      "Epoch: 68, loss: 0.0377561867130019\n",
      "Epoch: 69, loss: 0.03775610573757853\n",
      "Epoch: 70, loss: 0.03775602515703899\n",
      "Epoch: 71, loss: 0.037755945006395314\n",
      "Epoch: 72, loss: 0.037755865270908295\n",
      "Epoch: 73, loss: 0.03775578596363907\n",
      "Epoch: 74, loss: 0.03775570706877685\n",
      "Epoch: 75, loss: 0.03775562855157679\n",
      "Epoch: 76, loss: 0.03775555043379056\n",
      "Epoch: 77, loss: 0.03775547271132456\n",
      "Epoch: 78, loss: 0.03775539539375172\n",
      "Epoch: 73, loss: 0.03775578596363907\n",
      "Epoch: 74, loss: 0.03775570706877685\n",
      "Epoch: 75, loss: 0.03775562855157679\n",
      "Epoch: 76, loss: 0.03775555043379056\n",
      "Epoch: 77, loss: 0.03775547271132456\n",
      "Epoch: 78, loss: 0.03775539539375172\n",
      "Epoch: 79, loss: 0.037755318438715754\n",
      "Epoch: 80, loss: 0.037755241877080986\n",
      "Epoch: 81, loss: 0.03775516570617628\n",
      "Epoch: 82, loss: 0.03775508993150872\n",
      "Epoch: 83, loss: 0.03775501449780343\n",
      "Epoch: 84, loss: 0.03775493943403598\n",
      "Epoch: 79, loss: 0.037755318438715754\n",
      "Epoch: 80, loss: 0.037755241877080986\n",
      "Epoch: 81, loss: 0.03775516570617628\n",
      "Epoch: 82, loss: 0.03775508993150872\n",
      "Epoch: 83, loss: 0.03775501449780343\n",
      "Epoch: 84, loss: 0.03775493943403598\n",
      "Epoch: 85, loss: 0.03775486473647687\n",
      "Epoch: 86, loss: 0.03775479041536396\n",
      "Epoch: 87, loss: 0.037754716427801024\n",
      "Epoch: 88, loss: 0.03775464280593513\n",
      "Epoch: 89, loss: 0.03775456953970376\n",
      "Epoch: 90, loss: 0.03775449665709937\n",
      "Epoch: 91, loss: 0.03775442408119944\n",
      "Epoch: 92, loss: 0.03775435184858013\n",
      "Epoch: 85, loss: 0.03775486473647687\n",
      "Epoch: 86, loss: 0.03775479041536396\n",
      "Epoch: 87, loss: 0.037754716427801024\n",
      "Epoch: 88, loss: 0.03775464280593513\n",
      "Epoch: 89, loss: 0.03775456953970376\n",
      "Epoch: 90, loss: 0.03775449665709937\n",
      "Epoch: 91, loss: 0.03775442408119944\n",
      "Epoch: 92, loss: 0.03775435184858013\n",
      "Epoch: 93, loss: 0.03775427995586048\n",
      "Epoch: 94, loss: 0.037754208413938437\n",
      "Epoch: 95, loss: 0.03775413717928948\n",
      "Epoch: 96, loss: 0.037754066285370036\n",
      "Epoch: 97, loss: 0.03775399571922411\n",
      "Epoch: 98, loss: 0.03775392550749236\n",
      "Epoch: 99, loss: 0.03775385559750508\n",
      "Epoch: 100, loss: 0.03775378599419881\n",
      "Epoch: 93, loss: 0.03775427995586048\n",
      "Epoch: 94, loss: 0.037754208413938437\n",
      "Epoch: 95, loss: 0.03775413717928948\n",
      "Epoch: 96, loss: 0.037754066285370036\n",
      "Epoch: 97, loss: 0.03775399571922411\n",
      "Epoch: 98, loss: 0.03775392550749236\n",
      "Epoch: 99, loss: 0.03775385559750508\n",
      "Epoch: 100, loss: 0.03775378599419881\n",
      "Epoch: 101, loss: 0.0377537167071762\n",
      "Epoch: 102, loss: 0.037753647747954776\n",
      "Epoch: 103, loss: 0.0377535790721857\n",
      "Epoch: 104, loss: 0.03775351071470198\n",
      "Epoch: 105, loss: 0.03775344266241637\n",
      "Epoch: 106, loss: 0.037753374933647686\n",
      "Epoch: 101, loss: 0.0377537167071762\n",
      "Epoch: 102, loss: 0.037753647747954776\n",
      "Epoch: 103, loss: 0.0377535790721857\n",
      "Epoch: 104, loss: 0.03775351071470198\n",
      "Epoch: 105, loss: 0.03775344266241637\n",
      "Epoch: 106, loss: 0.037753374933647686\n",
      "Epoch: 107, loss: 0.03775330750171409\n",
      "Epoch: 108, loss: 0.03775324034621578\n",
      "Epoch: 109, loss: 0.03775317348548791\n",
      "Epoch: 110, loss: 0.03775310691678742\n",
      "Epoch: 111, loss: 0.037753040650571126\n",
      "Epoch: 112, loss: 0.03775297464701644\n",
      "Epoch: 113, loss: 0.03775290893720377\n",
      "Epoch: 114, loss: 0.03775284352144951\n",
      "Epoch: 107, loss: 0.03775330750171409\n",
      "Epoch: 108, loss: 0.03775324034621578\n",
      "Epoch: 109, loss: 0.03775317348548791\n",
      "Epoch: 110, loss: 0.03775310691678742\n",
      "Epoch: 111, loss: 0.037753040650571126\n",
      "Epoch: 112, loss: 0.03775297464701644\n",
      "Epoch: 113, loss: 0.03775290893720377\n",
      "Epoch: 114, loss: 0.03775284352144951\n",
      "Epoch: 115, loss: 0.03775195385993812\n",
      "Epoch: 116, loss: 0.03774992630230867\n",
      "Epoch: 117, loss: 0.03774859851334876\n",
      "Epoch: 118, loss: 0.037747891483052874\n",
      "Epoch: 119, loss: 0.03774740917568951\n",
      "Epoch: 120, loss: 0.03774706095949736\n",
      "Epoch: 121, loss: 0.037746801555849835\n",
      "Epoch: 122, loss: 0.03774660148960684\n",
      "Epoch: 123, loss: 0.037746441434279195\n",
      "Epoch: 115, loss: 0.03775195385993812\n",
      "Epoch: 116, loss: 0.03774992630230867\n",
      "Epoch: 117, loss: 0.03774859851334876\n",
      "Epoch: 118, loss: 0.037747891483052874\n",
      "Epoch: 119, loss: 0.03774740917568951\n",
      "Epoch: 120, loss: 0.03774706095949736\n",
      "Epoch: 121, loss: 0.037746801555849835\n",
      "Epoch: 122, loss: 0.03774660148960684\n",
      "Epoch: 123, loss: 0.037746441434279195\n",
      "Epoch: 124, loss: 0.037746308550795124\n",
      "Epoch: 125, loss: 0.0377461943196097\n",
      "Epoch: 126, loss: 0.03774609306208091\n",
      "Epoch: 127, loss: 0.03774600095820802\n",
      "Epoch: 128, loss: 0.03774591544872853\n",
      "Epoch: 129, loss: 0.03774583475832076\n",
      "Epoch: 124, loss: 0.037746308550795124\n",
      "Epoch: 125, loss: 0.0377461943196097\n",
      "Epoch: 126, loss: 0.03774609306208091\n",
      "Epoch: 127, loss: 0.03774600095820802\n",
      "Epoch: 128, loss: 0.03774591544872853\n",
      "Epoch: 129, loss: 0.03774583475832076\n",
      "Epoch: 130, loss: 0.037745757741663465\n",
      "Epoch: 131, loss: 0.03774568358537041\n",
      "Epoch: 132, loss: 0.037745611762992376\n",
      "Epoch: 133, loss: 0.03774554185265376\n",
      "Epoch: 134, loss: 0.03774547359148047\n",
      "Epoch: 135, loss: 0.03774540680365992\n",
      "Epoch: 136, loss: 0.03774534135348437\n",
      "Epoch: 137, loss: 0.037745277154244576\n",
      "Epoch: 130, loss: 0.037745757741663465\n",
      "Epoch: 131, loss: 0.03774568358537041\n",
      "Epoch: 132, loss: 0.037745611762992376\n",
      "Epoch: 133, loss: 0.03774554185265376\n",
      "Epoch: 134, loss: 0.03774547359148047\n",
      "Epoch: 135, loss: 0.03774540680365992\n",
      "Epoch: 136, loss: 0.03774534135348437\n",
      "Epoch: 137, loss: 0.037745277154244576\n",
      "Epoch: 138, loss: 0.03774521409348815\n",
      "Epoch: 139, loss: 0.0377451521470918\n",
      "Epoch: 140, loss: 0.03774509125902884\n",
      "Epoch: 141, loss: 0.03774503141036927\n",
      "Epoch: 142, loss: 0.03774497254288446\n",
      "Epoch: 143, loss: 0.03774491461627669\n",
      "Epoch: 144, loss: 0.03774485762150916\n",
      "Epoch: 145, loss: 0.037744801534386946\n",
      "Epoch: 146, loss: 0.037744746345783425\n",
      "Epoch: 138, loss: 0.03774521409348815\n",
      "Epoch: 139, loss: 0.0377451521470918\n",
      "Epoch: 140, loss: 0.03774509125902884\n",
      "Epoch: 141, loss: 0.03774503141036927\n",
      "Epoch: 142, loss: 0.03774497254288446\n",
      "Epoch: 143, loss: 0.03774491461627669\n",
      "Epoch: 144, loss: 0.03774485762150916\n",
      "Epoch: 145, loss: 0.037744801534386946\n",
      "Epoch: 146, loss: 0.037744746345783425\n",
      "Epoch: 147, loss: 0.037744691995981516\n",
      "Epoch: 148, loss: 0.037744638499896926\n",
      "Epoch: 149, loss: 0.037744585828406764\n",
      "Epoch: 150, loss: 0.03774453397154411\n",
      "Epoch: 151, loss: 0.03774448291808668\n",
      "Epoch: 152, loss: 0.0377444326076427\n",
      "Epoch: 153, loss: 0.03774438305038494\n",
      "Epoch: 147, loss: 0.037744691995981516\n",
      "Epoch: 148, loss: 0.037744638499896926\n",
      "Epoch: 149, loss: 0.037744585828406764\n",
      "Epoch: 150, loss: 0.03774453397154411\n",
      "Epoch: 151, loss: 0.03774448291808668\n",
      "Epoch: 152, loss: 0.0377444326076427\n",
      "Epoch: 153, loss: 0.03774438305038494\n",
      "Epoch: 154, loss: 0.03774433422945773\n",
      "Epoch: 155, loss: 0.03774428614259321\n",
      "Epoch: 156, loss: 0.037744238733488415\n",
      "Epoch: 157, loss: 0.037744192023628685\n",
      "Epoch: 158, loss: 0.037744145987603936\n",
      "Epoch: 159, loss: 0.037744100610530754\n",
      "Epoch: 160, loss: 0.03774405589959578\n",
      "Epoch: 154, loss: 0.03774433422945773\n",
      "Epoch: 155, loss: 0.03774428614259321\n",
      "Epoch: 156, loss: 0.037744238733488415\n",
      "Epoch: 157, loss: 0.037744192023628685\n",
      "Epoch: 158, loss: 0.037744145987603936\n",
      "Epoch: 159, loss: 0.037744100610530754\n",
      "Epoch: 160, loss: 0.03774405589959578\n",
      "Epoch: 161, loss: 0.03774401180863779\n",
      "Epoch: 162, loss: 0.037743968321076994\n",
      "Epoch: 163, loss: 0.037743925437163574\n",
      "Epoch: 164, loss: 0.03774388314363215\n",
      "Epoch: 165, loss: 0.03774384144115636\n",
      "Epoch: 166, loss: 0.03774380027843366\n",
      "Epoch: 167, loss: 0.03774375967848347\n",
      "Epoch: 168, loss: 0.03774371961959474\n",
      "Epoch: 161, loss: 0.03774401180863779\n",
      "Epoch: 162, loss: 0.037743968321076994\n",
      "Epoch: 163, loss: 0.037743925437163574\n",
      "Epoch: 164, loss: 0.03774388314363215\n",
      "Epoch: 165, loss: 0.03774384144115636\n",
      "Epoch: 166, loss: 0.03774380027843366\n",
      "Epoch: 167, loss: 0.03774375967848347\n",
      "Epoch: 168, loss: 0.03774371961959474\n",
      "Epoch: 169, loss: 0.037743680090033135\n",
      "Epoch: 170, loss: 0.037743641110591836\n",
      "Epoch: 171, loss: 0.03774360260341579\n",
      "Epoch: 172, loss: 0.0377435645923293\n",
      "Epoch: 173, loss: 0.037743527066516507\n",
      "Epoch: 174, loss: 0.037743490015455135\n",
      "Epoch: 175, loss: 0.03774345344225557\n",
      "Epoch: 176, loss: 0.03774341729897074\n",
      "Epoch: 177, loss: 0.03774338161030667\n",
      "Epoch: 169, loss: 0.037743680090033135\n",
      "Epoch: 170, loss: 0.037743641110591836\n",
      "Epoch: 171, loss: 0.03774360260341579\n",
      "Epoch: 172, loss: 0.0377435645923293\n",
      "Epoch: 173, loss: 0.037743527066516507\n",
      "Epoch: 174, loss: 0.037743490015455135\n",
      "Epoch: 175, loss: 0.03774345344225557\n",
      "Epoch: 176, loss: 0.03774341729897074\n",
      "Epoch: 177, loss: 0.03774338161030667\n",
      "Epoch: 178, loss: 0.03774334635726248\n",
      "Epoch: 179, loss: 0.03774331153210426\n",
      "Epoch: 180, loss: 0.037743277152533165\n",
      "Epoch: 181, loss: 0.037743243149063084\n",
      "Epoch: 182, loss: 0.03774320954543274\n",
      "Epoch: 183, loss: 0.03774317633299818\n",
      "Epoch: 184, loss: 0.0377431435033531\n",
      "Epoch: 185, loss: 0.03774311106172742\n",
      "Epoch: 178, loss: 0.03774334635726248\n",
      "Epoch: 179, loss: 0.03774331153210426\n",
      "Epoch: 180, loss: 0.037743277152533165\n",
      "Epoch: 181, loss: 0.037743243149063084\n",
      "Epoch: 182, loss: 0.03774320954543274\n",
      "Epoch: 183, loss: 0.03774317633299818\n",
      "Epoch: 184, loss: 0.0377431435033531\n",
      "Epoch: 185, loss: 0.03774311106172742\n",
      "Epoch: 186, loss: 0.03774307896211402\n",
      "Epoch: 187, loss: 0.03774304723119342\n",
      "Epoch: 188, loss: 0.03774301585186701\n",
      "Epoch: 189, loss: 0.03774298481659775\n",
      "Epoch: 190, loss: 0.03774295414283052\n",
      "Epoch: 191, loss: 0.03774292378024782\n",
      "Epoch: 186, loss: 0.03774307896211402\n",
      "Epoch: 187, loss: 0.03774304723119342\n",
      "Epoch: 188, loss: 0.03774301585186701\n",
      "Epoch: 189, loss: 0.03774298481659775\n",
      "Epoch: 190, loss: 0.03774295414283052\n",
      "Epoch: 191, loss: 0.03774292378024782\n",
      "Epoch: 192, loss: 0.037742621479650955\n",
      "Epoch: 193, loss: 0.037744071878504724\n",
      "Epoch: 194, loss: 0.03774379147952065\n",
      "Epoch: 195, loss: 0.037743223166793814\n",
      "Epoch: 196, loss: 0.03774315528850811\n",
      "Epoch: 197, loss: 0.037743542371623834\n",
      "Epoch: 198, loss: 0.03774335886361416\n",
      "Epoch: 192, loss: 0.037742621479650955\n",
      "Epoch: 193, loss: 0.037744071878504724\n",
      "Epoch: 194, loss: 0.03774379147952065\n",
      "Epoch: 195, loss: 0.037743223166793814\n",
      "Epoch: 196, loss: 0.03774315528850811\n",
      "Epoch: 197, loss: 0.037743542371623834\n",
      "Epoch: 198, loss: 0.03774335886361416\n",
      "Epoch: 199, loss: 0.03774297695556271\n",
      "Epoch: 200, loss: 0.03774420626721775\n",
      "Epoch: 201, loss: 0.03774372663645685\n",
      "Epoch: 202, loss: 0.03774317166537453\n",
      "Epoch: 203, loss: 0.03774371514806235\n",
      "Epoch: 204, loss: 0.037744544054018336\n",
      "Epoch: 205, loss: 0.03774370784586812\n",
      "Epoch: 206, loss: 0.03774381929167402\n",
      "Epoch: 199, loss: 0.03774297695556271\n",
      "Epoch: 200, loss: 0.03774420626721775\n",
      "Epoch: 201, loss: 0.03774372663645685\n",
      "Epoch: 202, loss: 0.03774317166537453\n",
      "Epoch: 203, loss: 0.03774371514806235\n",
      "Epoch: 204, loss: 0.037744544054018336\n",
      "Epoch: 205, loss: 0.03774370784586812\n",
      "Epoch: 206, loss: 0.03774381929167402\n",
      "Epoch: 207, loss: 0.03774388966131645\n",
      "Epoch: 208, loss: 0.037743930690412354\n",
      "Epoch: 209, loss: 0.03774395097056611\n",
      "Epoch: 210, loss: 0.03774395671404093\n",
      "Epoch: 211, loss: 0.03774393344397996\n",
      "Epoch: 212, loss: 0.037743892398844965\n",
      "Epoch: 213, loss: 0.03774463468108674\n",
      "Epoch: 207, loss: 0.03774388966131645\n",
      "Epoch: 208, loss: 0.037743930690412354\n",
      "Epoch: 209, loss: 0.03774395097056611\n",
      "Epoch: 210, loss: 0.03774395671404093\n",
      "Epoch: 211, loss: 0.03774393344397996\n",
      "Epoch: 212, loss: 0.037743892398844965\n",
      "Epoch: 213, loss: 0.03774463468108674\n",
      "Epoch: 214, loss: 0.03774389901890973\n",
      "Epoch: 215, loss: 0.03774382659946663\n",
      "Epoch: 216, loss: 0.0377440237590071\n",
      "Epoch: 217, loss: 0.03774387727012738\n",
      "Epoch: 218, loss: 0.03774447818745112\n",
      "Epoch: 219, loss: 0.03774401636455755\n",
      "Epoch: 220, loss: 0.03774383960111721\n",
      "Epoch: 214, loss: 0.03774389901890973\n",
      "Epoch: 215, loss: 0.03774382659946663\n",
      "Epoch: 216, loss: 0.0377440237590071\n",
      "Epoch: 217, loss: 0.03774387727012738\n",
      "Epoch: 218, loss: 0.03774447818745112\n",
      "Epoch: 219, loss: 0.03774401636455755\n",
      "Epoch: 220, loss: 0.03774383960111721\n",
      "Epoch: 221, loss: 0.03774395484179752\n",
      "Epoch: 222, loss: 0.03774376166367999\n",
      "Epoch: 223, loss: 0.0377444479824504\n",
      "Epoch: 224, loss: 0.03774386449461353\n",
      "Epoch: 225, loss: 0.03774391171151197\n",
      "Epoch: 226, loss: 0.0377444330286102\n",
      "Epoch: 221, loss: 0.03774395484179752\n",
      "Epoch: 222, loss: 0.03774376166367999\n",
      "Epoch: 223, loss: 0.0377444479824504\n",
      "Epoch: 224, loss: 0.03774386449461353\n",
      "Epoch: 225, loss: 0.03774391171151197\n",
      "Epoch: 226, loss: 0.0377444330286102\n",
      "Epoch: 227, loss: 0.037743906497015464\n",
      "Epoch: 228, loss: 0.03774389648132464\n",
      "Epoch: 229, loss: 0.03774441912684202\n",
      "Epoch: 230, loss: 0.037743849991637934\n",
      "Epoch: 231, loss: 0.037743819655703045\n",
      "Epoch: 232, loss: 0.037744403209729785\n",
      "Epoch: 227, loss: 0.037743906497015464\n",
      "Epoch: 228, loss: 0.03774389648132464\n",
      "Epoch: 229, loss: 0.03774441912684202\n",
      "Epoch: 230, loss: 0.037743849991637934\n",
      "Epoch: 231, loss: 0.037743819655703045\n",
      "Epoch: 232, loss: 0.037744403209729785\n",
      "Epoch: 233, loss: 0.0377437595136733\n",
      "Epoch: 234, loss: 0.03774372235444104\n",
      "Epoch: 235, loss: 0.037744386696373165\n",
      "Epoch: 236, loss: 0.037743874355548976\n",
      "Epoch: 237, loss: 0.03774438817192593\n",
      "Epoch: 238, loss: 0.03774371400595864\n",
      "Epoch: 233, loss: 0.0377437595136733\n",
      "Epoch: 234, loss: 0.03774372235444104\n",
      "Epoch: 235, loss: 0.037744386696373165\n",
      "Epoch: 236, loss: 0.037743874355548976\n",
      "Epoch: 237, loss: 0.03774438817192593\n",
      "Epoch: 238, loss: 0.03774371400595864\n",
      "Epoch: 239, loss: 0.03774440156772166\n",
      "Epoch: 240, loss: 0.03774379416606662\n",
      "Epoch: 241, loss: 0.037744404677198605\n",
      "Epoch: 242, loss: 0.037743804891352546\n",
      "Epoch: 243, loss: 0.037744399846600905\n",
      "Epoch: 244, loss: 0.03774377975595935\n",
      "Epoch: 245, loss: 0.037744390344623444\n",
      "Epoch: 239, loss: 0.03774440156772166\n",
      "Epoch: 240, loss: 0.03774379416606662\n",
      "Epoch: 241, loss: 0.037744404677198605\n",
      "Epoch: 242, loss: 0.037743804891352546\n",
      "Epoch: 243, loss: 0.037744399846600905\n",
      "Epoch: 244, loss: 0.03774377975595935\n",
      "Epoch: 245, loss: 0.037744390344623444\n",
      "Epoch: 246, loss: 0.037743736635337234\n",
      "Epoch: 247, loss: 0.03774437842175193\n",
      "Epoch: 248, loss: 0.03774368474552449\n",
      "Epoch: 249, loss: 0.03774436527125615\n",
      "Epoch: 250, loss: 0.037743628758836155\n",
      "Epoch: 251, loss: 0.03774435157461689\n",
      "Epoch: 246, loss: 0.037743736635337234\n",
      "Epoch: 247, loss: 0.03774437842175193\n",
      "Epoch: 248, loss: 0.03774368474552449\n",
      "Epoch: 249, loss: 0.03774436527125615\n",
      "Epoch: 250, loss: 0.037743628758836155\n",
      "Epoch: 251, loss: 0.03774435157461689\n",
      "Epoch: 252, loss: 0.03774376243603645\n",
      "Epoch: 253, loss: 0.03774431638914928\n",
      "Epoch: 254, loss: 0.03774439140346845\n",
      "Epoch: 255, loss: 0.037743702133742334\n",
      "Epoch: 256, loss: 0.03774434239911651\n",
      "Epoch: 257, loss: 0.03774372448128039\n",
      "Epoch: 252, loss: 0.03774376243603645\n",
      "Epoch: 253, loss: 0.03774431638914928\n",
      "Epoch: 254, loss: 0.03774439140346845\n",
      "Epoch: 255, loss: 0.037743702133742334\n",
      "Epoch: 256, loss: 0.03774434239911651\n",
      "Epoch: 257, loss: 0.03774372448128039\n",
      "Epoch: 258, loss: 0.037744289646528324\n",
      "Epoch: 259, loss: 0.0377443640675164\n",
      "Epoch: 260, loss: 0.03774360383962199\n",
      "Epoch: 261, loss: 0.03774430782661646\n",
      "Epoch: 262, loss: 0.03774438110295196\n",
      "Epoch: 263, loss: 0.03774364188932578\n",
      "Epoch: 258, loss: 0.037744289646528324\n",
      "Epoch: 259, loss: 0.0377443640675164\n",
      "Epoch: 260, loss: 0.03774360383962199\n",
      "Epoch: 261, loss: 0.03774430782661646\n",
      "Epoch: 262, loss: 0.03774438110295196\n",
      "Epoch: 263, loss: 0.03774364188932578\n",
      "Epoch: 264, loss: 0.037744300101514534\n",
      "Epoch: 265, loss: 0.03774437122205212\n",
      "Epoch: 266, loss: 0.03774360721556771\n",
      "Epoch: 267, loss: 0.03774428067472368\n",
      "Epoch: 268, loss: 0.03774435041820219\n",
      "Epoch: 269, loss: 0.03774354597492642\n",
      "Epoch: 264, loss: 0.037744300101514534\n",
      "Epoch: 265, loss: 0.03774437122205212\n",
      "Epoch: 266, loss: 0.03774360721556771\n",
      "Epoch: 267, loss: 0.03774428067472368\n",
      "Epoch: 268, loss: 0.03774435041820219\n",
      "Epoch: 269, loss: 0.03774354597492642\n",
      "Epoch: 270, loss: 0.03774425674641354\n",
      "Epoch: 271, loss: 0.03774432551051273\n",
      "Epoch: 272, loss: 0.037743643805662516\n",
      "Epoch: 273, loss: 0.037744204110541774\n",
      "Epoch: 274, loss: 0.037744274177716905\n",
      "Epoch: 275, loss: 0.037744334763679534\n",
      "Epoch: 276, loss: 0.037743495518351586\n",
      "Epoch: 270, loss: 0.03774425674641354\n",
      "Epoch: 271, loss: 0.03774432551051273\n",
      "Epoch: 272, loss: 0.037743643805662516\n",
      "Epoch: 273, loss: 0.037744204110541774\n",
      "Epoch: 274, loss: 0.037744274177716905\n",
      "Epoch: 275, loss: 0.037744334763679534\n",
      "Epoch: 276, loss: 0.037743495518351586\n",
      "Epoch: 277, loss: 0.03774421202967885\n",
      "Epoch: 278, loss: 0.0377442770144868\n",
      "Epoch: 279, loss: 0.03774432962670361\n",
      "Epoch: 280, loss: 0.037743478801750885\n",
      "Epoch: 281, loss: 0.037744186797619435\n",
      "Epoch: 282, loss: 0.037744249229183596\n",
      "Epoch: 277, loss: 0.03774421202967885\n",
      "Epoch: 278, loss: 0.0377442770144868\n",
      "Epoch: 279, loss: 0.03774432962670361\n",
      "Epoch: 280, loss: 0.037743478801750885\n",
      "Epoch: 281, loss: 0.037744186797619435\n",
      "Epoch: 282, loss: 0.037744249229183596\n",
      "Epoch: 283, loss: 0.037744299060089394\n",
      "Epoch: 284, loss: 0.037743563910218034\n",
      "Epoch: 285, loss: 0.03774412203448708\n",
      "Epoch: 286, loss: 0.03774418678018233\n",
      "Epoch: 287, loss: 0.03774424029045159\n",
      "Epoch: 288, loss: 0.03774428070531323\n",
      "Epoch: 283, loss: 0.037744299060089394\n",
      "Epoch: 284, loss: 0.037743563910218034\n",
      "Epoch: 285, loss: 0.03774412203448708\n",
      "Epoch: 286, loss: 0.03774418678018233\n",
      "Epoch: 287, loss: 0.03774424029045159\n",
      "Epoch: 288, loss: 0.03774428070531323\n",
      "Epoch: 289, loss: 0.037744309101744894\n",
      "Epoch: 290, loss: 0.037743432252308196\n",
      "Epoch: 291, loss: 0.03774411666863965\n",
      "Epoch: 292, loss: 0.037744172537136404\n",
      "Epoch: 293, loss: 0.03774421566989907\n",
      "Epoch: 294, loss: 0.03774424662407289\n",
      "Epoch: 295, loss: 0.037744267155413166\n",
      "Epoch: 296, loss: 0.03774349821647597\n",
      "Epoch: 289, loss: 0.037744309101744894\n",
      "Epoch: 290, loss: 0.037743432252308196\n",
      "Epoch: 291, loss: 0.03774411666863965\n",
      "Epoch: 292, loss: 0.037744172537136404\n",
      "Epoch: 293, loss: 0.03774421566989907\n",
      "Epoch: 294, loss: 0.03774424662407289\n",
      "Epoch: 295, loss: 0.037744267155413166\n",
      "Epoch: 296, loss: 0.03774349821647597\n",
      "Epoch: 297, loss: 0.03774403009701477\n",
      "Epoch: 298, loss: 0.03774408846889633\n",
      "Epoch: 299, loss: 0.0377441348399344\n",
      "Epoch: 300, loss: 0.037744168891047924\n",
      "Epoch: 297, loss: 0.03774403009701477\n",
      "Epoch: 298, loss: 0.03774408846889633\n",
      "Epoch: 299, loss: 0.0377441348399344\n",
      "Epoch: 300, loss: 0.037744168891047924\n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras/TensorFlow/PyTorch etc. these are usually randomized in the beginning\n",
    "w1 = np.random.normal(0, 1.5)\n",
    "w2 = np.random.normal(0, 1.5)\n",
    "w3 = np.random.normal(0, 1.5)\n",
    "w4 = np.random.normal(0, 1.5)\n",
    "w5 = np.random.normal(0, 1.5)\n",
    "w6 = np.random.normal(0, 1.5)\n",
    "bias1 = np.random.normal(0, 1.5)\n",
    "bias2 = np.random.normal(0, 1.5)\n",
    "bias3 = np.random.normal(0, 1.5)\n",
    "\n",
    "# just for comparison after the training\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    "\n",
    "# DataFrame values as a list\n",
    "# data = list(df.values)\n",
    "data = df.values\n",
    "\n",
    "# use min/max -scaling to make values in the range 0-1\n",
    "\n",
    "# independent support variables => indeces 0 and 1 (X)\n",
    "# target variable => index 2 (y)\n",
    "X = data[:, :2]\n",
    "y = data[:, 2:]\n",
    "\n",
    "# Scale X and y\n",
    "X_scaled = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))\n",
    "y_scaled = (y - np.min(y, axis=0)) / (np.max(y, axis=0) - np.min(y, axis=0))\n",
    "\n",
    "# combine back to original data format\n",
    "data = np.hstack((X_scaled, y_scaled))\n",
    "\n",
    "\n",
    "\n",
    "# learning rate\n",
    "LR = 0.005\n",
    "epochs = 300\n",
    "\n",
    "# let's initalize a list for loss visualizations\n",
    "loss_points = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # the previous version only measured the loss value\n",
    "    # of the last calculation done in the code (node 3)\n",
    "    # it's probably better to measure the average loss for each epoch\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # this is where we do Forward pass + backpropagation\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        pre_node1 = input1 * w1 + input2 * w3 + bias1\n",
    "        pre_node2 = input1 * w2 + input2 * w4 + bias2\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = activation_ReLu(pre_node1)\n",
    "        node_1_output\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = activation_ReLu(pre_node2)\n",
    "        node_2_output\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the previous weights in their result\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        node_3_output\n",
    "\n",
    "        # LOSS FUNCTION - we are going to use MSE -> mean squared error\n",
    "        # MSE formula for LOSS => (predicted_value - true_value) ^ 2\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current loss into epoch losses -list\n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solving the partial derivative of the loss function with respect to w5\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT ONWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES => CHAIN RULE\n",
    "\n",
    "        # weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(pre_node1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(pre_node2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(pre_node2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(pre_node2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it to loss points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the overall epoch loss into the loss_points list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch + 1}, loss: {average_loss}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "# plt.ylim(-1, 5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 2.093029394635432\n",
      "w2: -0.5384038893633919\n",
      "w3: -0.8229631922792975\n",
      "w4: -3.8355819061017877\n",
      "w5: -0.8233806199833301\n",
      "w6: -1.4670865591264255\n",
      "b1: -0.532236687142228\n",
      "b2: 0.5873763635251666\n",
      "b3: 0.26578849379848185\n",
      "\n",
      "\n",
      "#################################\n",
      "\n",
      "\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.432337674416812\n",
      "w2: -0.36985185551358646\n",
      "w3: -1.0692113954409057\n",
      "w4: -3.7445529766395422\n",
      "w5: -0.2470135634622536\n",
      "w6: -1.0172445915713295\n",
      "b1: -1.2372961762045476\n",
      "b2: 0.29447021375399324\n",
      "b3: 0.19440604236398712\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n#################################\\n\\n\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {new_w1}\")\n",
    "print(f\"w2: {new_w2}\")\n",
    "print(f\"w3: {new_w3}\")\n",
    "print(f\"w4: {new_w4}\")\n",
    "print(f\"w5: {new_w5}\")\n",
    "print(f\"w6: {new_w6}\")\n",
    "print(f\"b1: {new_b1}\")\n",
    "print(f\"b2: {new_b2}\")\n",
    "print(f\"b3: {new_b3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the previous weights in their result\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "    return node_3_output\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bmi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "charges",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a27efa6e-8300-4ef1-9012-679008772fbe",
       "rows": [
        [
         "0",
         "19",
         "27.9",
         "16884.924"
        ],
        [
         "1",
         "18",
         "33.77",
         "1725.5523"
        ],
        [
         "2",
         "28",
         "33.0",
         "4449.462"
        ],
        [
         "3",
         "33",
         "22.705",
         "21984.47061"
        ],
        [
         "4",
         "32",
         "28.88",
         "3866.8552"
        ],
        [
         "5",
         "31",
         "25.74",
         "3756.6216"
        ],
        [
         "6",
         "46",
         "33.44",
         "8240.5896"
        ],
        [
         "7",
         "37",
         "27.74",
         "7281.5056"
        ],
        [
         "8",
         "37",
         "29.83",
         "6406.4107"
        ],
        [
         "9",
         "60",
         "25.84",
         "28923.13692"
        ],
        [
         "10",
         "25",
         "26.22",
         "2721.3208"
        ],
        [
         "11",
         "62",
         "26.29",
         "27808.7251"
        ],
        [
         "12",
         "23",
         "34.4",
         "1826.843"
        ],
        [
         "13",
         "56",
         "39.82",
         "11090.7178"
        ],
        [
         "14",
         "27",
         "42.13",
         "39611.7577"
        ],
        [
         "15",
         "19",
         "24.6",
         "1837.237"
        ],
        [
         "16",
         "52",
         "30.78",
         "10797.3362"
        ],
        [
         "17",
         "23",
         "23.845",
         "2395.17155"
        ],
        [
         "18",
         "56",
         "40.3",
         "10602.385"
        ],
        [
         "19",
         "30",
         "35.3",
         "36837.467"
        ],
        [
         "20",
         "60",
         "36.005",
         "13228.84695"
        ],
        [
         "21",
         "30",
         "32.4",
         "4149.736"
        ],
        [
         "22",
         "18",
         "34.1",
         "1137.011"
        ],
        [
         "23",
         "34",
         "31.92",
         "37701.8768"
        ],
        [
         "24",
         "37",
         "28.025",
         "6203.90175"
        ],
        [
         "25",
         "59",
         "27.72",
         "14001.1338"
        ],
        [
         "26",
         "63",
         "23.085",
         "14451.83515"
        ],
        [
         "27",
         "55",
         "32.775",
         "12268.63225"
        ],
        [
         "28",
         "23",
         "17.385",
         "2775.19215"
        ],
        [
         "29",
         "31",
         "36.3",
         "38711.0"
        ],
        [
         "30",
         "22",
         "35.6",
         "35585.576"
        ],
        [
         "31",
         "18",
         "26.315",
         "2198.18985"
        ],
        [
         "32",
         "19",
         "28.6",
         "4687.797"
        ],
        [
         "33",
         "63",
         "28.31",
         "13770.0979"
        ],
        [
         "34",
         "28",
         "36.4",
         "51194.55914"
        ],
        [
         "35",
         "19",
         "20.425",
         "1625.43375"
        ],
        [
         "36",
         "62",
         "32.965",
         "15612.19335"
        ],
        [
         "37",
         "26",
         "20.8",
         "2302.3"
        ],
        [
         "38",
         "35",
         "36.67",
         "39774.2763"
        ],
        [
         "39",
         "60",
         "39.9",
         "48173.361"
        ],
        [
         "40",
         "24",
         "26.6",
         "3046.062"
        ],
        [
         "41",
         "31",
         "36.63",
         "4949.7587"
        ],
        [
         "42",
         "41",
         "21.78",
         "6272.4772"
        ],
        [
         "43",
         "37",
         "30.8",
         "6313.759"
        ],
        [
         "44",
         "38",
         "37.05",
         "6079.6715"
        ],
        [
         "45",
         "55",
         "37.3",
         "20630.28351"
        ],
        [
         "46",
         "18",
         "38.665",
         "3393.35635"
        ],
        [
         "47",
         "28",
         "34.77",
         "3556.9223"
        ],
        [
         "48",
         "60",
         "24.53",
         "12629.8967"
        ],
        [
         "49",
         "36",
         "35.2",
         "38709.176"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 2772
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index 543 is one of the biggest charges in the data\n",
    "#df.iloc[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=12, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled values\n",
    "#data[543]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(63770.42801)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# $ ~63770 \n",
    "df['charges'].max() * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.19440604236398712)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try using the model with our prediction function\n",
    "# the value tends to be same as final bias 3\n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.7826087 , 0.84611246)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(12397.356529281651)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['charges'].max() * result\n",
    "\n",
    "# estimated $ ~22335 USD, heavily undershoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bmi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "charges",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "7230ad89-e037-4d36-80a7-094da0a24d43",
       "rows": [
        [
         "count",
         "2772.0",
         "2772.0",
         "2772.0"
        ],
        [
         "mean",
         "39.10966810966811",
         "30.70134920634921",
         "13261.369959046897"
        ],
        [
         "std",
         "14.081459420836477",
         "6.1294486949652205",
         "12151.768945168045"
        ],
        [
         "min",
         "18.0",
         "15.96",
         "1121.8739"
        ],
        [
         "25%",
         "26.0",
         "26.22",
         "4687.797"
        ],
        [
         "50%",
         "39.0",
         "30.447499999999998",
         "9333.014350000001"
        ],
        [
         "75%",
         "51.0",
         "34.77",
         "16577.7795"
        ],
        [
         "max",
         "64.0",
         "53.13",
         "63770.42801"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.109668</td>\n",
       "      <td>30.701349</td>\n",
       "      <td>13261.369959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.081459</td>\n",
       "      <td>6.129449</td>\n",
       "      <td>12151.768945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>1121.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>4687.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.447500</td>\n",
       "      <td>9333.014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>16577.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.130000</td>\n",
       "      <td>63770.428010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          bmi       charges\n",
       "count  2772.000000  2772.000000   2772.000000\n",
       "mean     39.109668    30.701349  13261.369959\n",
       "std      14.081459     6.129449  12151.768945\n",
       "min      18.000000    15.960000   1121.873900\n",
       "25%      26.000000    26.220000   4687.797000\n",
       "50%      39.000000    30.447500   9333.014350\n",
       "75%      51.000000    34.770000  16577.779500\n",
       "max      64.000000    53.130000  63770.428010"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just checking if the value revolves around the average...\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "============================================================\n",
      "\n",
      "SCALED VALUES (0-1 range):\n",
      "  MSE (Mean Squared Error):  0.037479\n",
      "  RMSE (Root Mean Squared Error): 0.193595\n",
      "  MAE (Mean Absolute Error): 0.145397\n",
      "\n",
      "ORIGINAL VALUES (Insurance charges in USD):\n",
      "  MSE:  $147,098,568.29\n",
      "  RMSE: $12,128.42\n",
      "  MAE:  $9,108.88\n",
      "\n",
      "MODEL QUALITY METRICS:\n",
      "  RÂ² Score: 0.003480\n",
      "  MAPE (Mean Absolute Percentage Error): 152.21%\n",
      "\n",
      "DATA STATISTICS:\n",
      "  Mean charge: $13,261.37\n",
      "  Min charge:  $1,121.87\n",
      "  Max charge:  $63,770.43\n",
      "  Std Dev:     $12,149.58\n",
      "\n",
      "PREDICTION STATISTICS:\n",
      "  Mean prediction: $13,239.18\n",
      "  Min prediction:  $1,121.87\n",
      "  Max prediction:  $13,301.13\n",
      "  Std Dev:         $716.04\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for all data points and calculate metrics\n",
    "predictions_scaled = []\n",
    "true_values_scaled = []\n",
    "\n",
    "test_data = data[int(len(data)*0.75):]\n",
    "\n",
    "for row in data:\n",
    "    input1 = row[0]\n",
    "    input2 = row[1]\n",
    "    true_value = row[2]\n",
    "    \n",
    "    prediction = predict(input1, input2)\n",
    "    predictions_scaled.append(prediction)\n",
    "    true_values_scaled.append(true_value)\n",
    "\n",
    "predictions_scaled = np.array(predictions_scaled)\n",
    "true_values_scaled = np.array(true_values_scaled)\n",
    "\n",
    "# Denormalize predictions and true values back to original scale\n",
    "y_min = np.min(y)\n",
    "y_max = np.max(y)\n",
    "\n",
    "predictions_original = predictions_scaled * (y_max - y_min) + y_min\n",
    "true_values_original = true_values_scaled * (y_max - y_min) + y_min\n",
    "\n",
    "# Calculate metrics\n",
    "mse_scaled = np.mean((predictions_scaled - true_values_scaled) ** 2)\n",
    "rmse_scaled = np.sqrt(mse_scaled)\n",
    "mae_scaled = np.mean(np.abs(predictions_scaled - true_values_scaled))\n",
    "\n",
    "mse_original = np.mean((predictions_original - true_values_original) ** 2)\n",
    "rmse_original = np.sqrt(mse_original)\n",
    "mae_original = np.mean(np.abs(predictions_original - true_values_original))\n",
    "\n",
    "# RÂ² Score\n",
    "ss_res = np.sum((true_values_original - predictions_original) ** 2)\n",
    "ss_tot = np.sum((true_values_original - np.mean(true_values_original)) ** 2)\n",
    "r2_score = 1 - (ss_res / ss_tot)\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((true_values_original - predictions_original) / true_values_original)) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSCALED VALUES (0-1 range):\")\n",
    "print(f\"  MSE (Mean Squared Error):  {mse_scaled:.6f}\")\n",
    "print(f\"  RMSE (Root Mean Squared Error): {rmse_scaled:.6f}\")\n",
    "print(f\"  MAE (Mean Absolute Error): {mae_scaled:.6f}\")\n",
    "\n",
    "print(\"\\nORIGINAL VALUES (Insurance charges in USD):\")\n",
    "print(f\"  MSE:  ${mse_original:,.2f}\")\n",
    "print(f\"  RMSE: ${rmse_original:,.2f}\")\n",
    "print(f\"  MAE:  ${mae_original:,.2f}\")\n",
    "\n",
    "print(\"\\nMODEL QUALITY METRICS:\")\n",
    "print(f\"  RÂ² Score: {r2_score:.6f}\")\n",
    "print(f\"  MAPE (Mean Absolute Percentage Error): {mape:.2f}%\")\n",
    "\n",
    "print(\"\\nDATA STATISTICS:\")\n",
    "print(f\"  Mean charge: ${np.mean(true_values_original):,.2f}\")\n",
    "print(f\"  Min charge:  ${np.min(true_values_original):,.2f}\")\n",
    "print(f\"  Max charge:  ${np.max(true_values_original):,.2f}\")\n",
    "print(f\"  Std Dev:     ${np.std(true_values_original):,.2f}\")\n",
    "\n",
    "print(\"\\nPREDICTION STATISTICS:\")\n",
    "print(f\"  Mean prediction: ${np.mean(predictions_original):,.2f}\")\n",
    "print(f\"  Min prediction:  ${np.min(predictions_original):,.2f}\")\n",
    "print(f\"  Max prediction:  ${np.max(predictions_original):,.2f}\")\n",
    "print(f\"  Std Dev:         ${np.std(predictions_original):,.2f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Model - Same Architecture for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsolt\\OneDrive - Ã“budai egyetem\\Deep Learning\\deeplearning\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model training complete!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Build Keras model with same architecture as handmade model\n",
    "# Input: 2 neurons (age, bmi)\n",
    "# Hidden layer: 2 neurons with ReLU activation\n",
    "# Output: 1 neuron with ReLU activation\n",
    "keras_model = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu', input_shape=(2,)),  # Hidden layer with 2 neurons\n",
    "    layers.Dense(1, activation='relu')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile with MSE loss and Adam optimizer (similar learning rate)\n",
    "keras_model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.005)\n",
    ")\n",
    "\n",
    "# Prepare the data for Keras\n",
    "X_train = X_scaled  # Shape: (2772, 2)\n",
    "y_train = y_scaled  # Shape: (2772, 1)\n",
    "\n",
    "# Train the model for the same number of epochs\n",
    "# Using batch_size=32 for practical training speed\n",
    "history = keras_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=300,\n",
    "    batch_size=32,  # Use batches for faster training\n",
    "    verbose=0  # Suppress output\n",
    ")\n",
    "\n",
    "print(\"Keras model training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KERAS MODEL PERFORMANCE METRICS\n",
      "============================================================\n",
      "\n",
      "SCALED VALUES (0-1 range):\n",
      "  MSE (Mean Squared Error):  0.033240\n",
      "  RMSE (Root Mean Squared Error): 0.182317\n",
      "  MAE (Mean Absolute Error): 0.142523\n",
      "\n",
      "ORIGINAL VALUES (Insurance charges in USD):\n",
      "  MSE:  $130,460,181.80\n",
      "  RMSE: $11,421.92\n",
      "  MAE:  $8,928.87\n",
      "\n",
      "MODEL QUALITY METRICS:\n",
      "  RÂ² Score: 0.116197\n",
      "  MAPE (Mean Absolute Percentage Error): 114.17%\n",
      "\n",
      "PREDICTION STATISTICS:\n",
      "  Mean prediction: $12,950.27\n",
      "  Min prediction:  $5,834.57\n",
      "  Max prediction:  $23,529.87\n",
      "  Std Dev:         $4,046.69\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Get predictions from Keras model\n",
    "keras_predictions_scaled = keras_model.predict(X_train, verbose=0)\n",
    "keras_predictions_scaled = keras_predictions_scaled.flatten()\n",
    "\n",
    "# Denormalize Keras predictions\n",
    "keras_predictions_original = keras_predictions_scaled * (y_max - y_min) + y_min\n",
    "\n",
    "# Calculate Keras metrics\n",
    "keras_mse_scaled = np.mean((keras_predictions_scaled - true_values_scaled) ** 2)\n",
    "keras_rmse_scaled = np.sqrt(keras_mse_scaled)\n",
    "keras_mae_scaled = np.mean(np.abs(keras_predictions_scaled - true_values_scaled))\n",
    "\n",
    "keras_mse_original = np.mean((keras_predictions_original - true_values_original) ** 2)\n",
    "keras_rmse_original = np.sqrt(keras_mse_original)\n",
    "keras_mae_original = np.mean(np.abs(keras_predictions_original - true_values_original))\n",
    "\n",
    "# RÂ² Score for Keras\n",
    "keras_ss_res = np.sum((true_values_original - keras_predictions_original) ** 2)\n",
    "keras_ss_tot = np.sum((true_values_original - np.mean(true_values_original)) ** 2)\n",
    "keras_r2_score = 1 - (keras_ss_res / keras_ss_tot)\n",
    "\n",
    "# MAPE for Keras\n",
    "keras_mape = np.mean(np.abs((true_values_original - keras_predictions_original) / true_values_original)) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KERAS MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSCALED VALUES (0-1 range):\")\n",
    "print(f\"  MSE (Mean Squared Error):  {keras_mse_scaled:.6f}\")\n",
    "print(f\"  RMSE (Root Mean Squared Error): {keras_rmse_scaled:.6f}\")\n",
    "print(f\"  MAE (Mean Absolute Error): {keras_mae_scaled:.6f}\")\n",
    "\n",
    "print(\"\\nORIGINAL VALUES (Insurance charges in USD):\")\n",
    "print(f\"  MSE:  ${keras_mse_original:,.2f}\")\n",
    "print(f\"  RMSE: ${keras_rmse_original:,.2f}\")\n",
    "print(f\"  MAE:  ${keras_mae_original:,.2f}\")\n",
    "\n",
    "print(\"\\nMODEL QUALITY METRICS:\")\n",
    "print(f\"  RÂ² Score: {keras_r2_score:.6f}\")\n",
    "print(f\"  MAPE (Mean Absolute Percentage Error): {keras_mape:.2f}%\")\n",
    "\n",
    "print(\"\\nPREDICTION STATISTICS:\")\n",
    "print(f\"  Mean prediction: ${np.mean(keras_predictions_original):,.2f}\")\n",
    "print(f\"  Min prediction:  ${np.min(keras_predictions_original):,.2f}\")\n",
    "print(f\"  Max prediction:  ${np.max(keras_predictions_original):,.2f}\")\n",
    "print(f\"  Std Dev:         ${np.std(keras_predictions_original):,.2f}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==========================================================================================\n",
      "DETAILED COMPARISON: HANDMADE vs KERAS MODEL\n",
      "==========================================================================================\n",
      "             Metric  Handmade Model     Keras Model\n",
      "       MSE (scaled)        0.037479        0.033240\n",
      "      RMSE (scaled)        0.193595        0.182317\n",
      "       MAE (scaled)        0.145397        0.142523\n",
      "     MSE (original) $147,098,568.29 $130,460,181.80\n",
      "    RMSE (original)      $12,128.42      $11,421.92\n",
      "     MAE (original)       $9,108.88       $8,928.87\n",
      "           RÂ² Score        0.003480        0.116197\n",
      "           MAPE (%)         152.21%         114.17%\n",
      "Mean Prediction ($)      $13,239.18      $12,950.27\n",
      "        Std Dev ($)         $716.04       $4,046.69\n",
      "==========================================================================================\n",
      "\n",
      "KERAS IMPROVEMENTS OVER HANDMADE:\n",
      "  RÂ² Score improvement: +0.112717\n",
      "  RMSE improvement: +5.83%\n",
      "  MAE improvement: +1.98%\n",
      "  MAPE improvement: +24.99%\n"
     ]
    }
   ],
   "source": [
    "## Comparison: Handmade vs Keras Model\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'MSE (scaled)',\n",
    "        'RMSE (scaled)',\n",
    "        'MAE (scaled)',\n",
    "        'MSE (original)',\n",
    "        'RMSE (original)',\n",
    "        'MAE (original)',\n",
    "        'RÂ² Score',\n",
    "        'MAPE (%)',\n",
    "        'Mean Prediction ($)',\n",
    "        'Std Dev ($)'\n",
    "    ],\n",
    "    'Handmade Model': [\n",
    "        f\"{mse_scaled:.6f}\",\n",
    "        f\"{rmse_scaled:.6f}\",\n",
    "        f\"{mae_scaled:.6f}\",\n",
    "        f\"${mse_original:,.2f}\",\n",
    "        f\"${rmse_original:,.2f}\",\n",
    "        f\"${mae_original:,.2f}\",\n",
    "        f\"{r2_score:.6f}\",\n",
    "        f\"{mape:.2f}%\",\n",
    "        f\"${np.mean(predictions_original):,.2f}\",\n",
    "        f\"${np.std(predictions_original):,.2f}\"\n",
    "    ],\n",
    "    'Keras Model': [\n",
    "        f\"{keras_mse_scaled:.6f}\",\n",
    "        f\"{keras_rmse_scaled:.6f}\",\n",
    "        f\"{keras_mae_scaled:.6f}\",\n",
    "        f\"${keras_mse_original:,.2f}\",\n",
    "        f\"${keras_rmse_original:,.2f}\",\n",
    "        f\"${keras_mae_original:,.2f}\",\n",
    "        f\"{keras_r2_score:.6f}\",\n",
    "        f\"{keras_mape:.2f}%\",\n",
    "        f\"${np.mean(keras_predictions_original):,.2f}\",\n",
    "        f\"${np.std(keras_predictions_original):,.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "print(\"=\" * 90)\n",
    "print(\"DETAILED COMPARISON: HANDMADE vs KERAS MODEL\")\n",
    "print(\"=\" * 90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calculate improvements\n",
    "print(\"\\nKERAS IMPROVEMENTS OVER HANDMADE:\")\n",
    "r2_improvement = keras_r2_score - r2_score\n",
    "rmse_improvement_pct = ((rmse_original - keras_rmse_original) / rmse_original) * 100\n",
    "mae_improvement_pct = ((mae_original - keras_mae_original) / mae_original) * 100\n",
    "mape_improvement_pct = ((mape - keras_mape) / mape) * 100\n",
    "\n",
    "print(f\"  RÂ² Score improvement: {r2_improvement:+.6f}\")\n",
    "print(f\"  RMSE improvement: {rmse_improvement_pct:+.2f}%\")\n",
    "print(f\"  MAE improvement: {mae_improvement_pct:+.2f}%\")\n",
    "print(f\"  MAPE improvement: {mape_improvement_pct:+.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
