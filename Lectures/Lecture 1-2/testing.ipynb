{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "312119ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teszt\n"
     ]
    }
   ],
   "source": [
    "print(\"teszt\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "teszt = np.abs(-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349ecc5",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub[pandas-datasets] in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (25.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (2.32.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from kagglehub[pandas-datasets]) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from pandas->kagglehub[pandas-datasets]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from requests->kagglehub[pandas-datasets]) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from requests->kagglehub[pandas-datasets]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from requests->kagglehub[pandas-datasets]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from requests->kagglehub[pandas-datasets]) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\zsolt\\onedrive - óbudai egyetem\\deep learning\\deeplearning\\.venv\\lib\\site-packages (from tqdm->kagglehub[pandas-datasets]) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zsolt\\AppData\\Local\\Temp\\ipykernel_20844\\3087723853.py:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
      "  df = kagglehub.load_dataset(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the latest version\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df = \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m  \u001b[49m\u001b[43mKaggleDatasetAdapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPANDAS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m  \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmrsimple07/restaurants-revenue-prediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m  \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide any additional arguments like \u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# sql_query or pandas_kwargs. See the \u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# documenation for more information:\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFirst 5 records:\u001b[39m\u001b[33m\"\u001b[39m, df.head())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zsolt\\OneDrive - Óbudai egyetem\\Deep Learning\\deeplearning\\.venv\\Lib\\site-packages\\kagglehub\\datasets.py:178\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_dataset\u001b[39m(\n\u001b[32m    164\u001b[39m     adapter: KaggleDatasetAdapter,\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# In the form of {owner_slug}/{dataset_slug} or {owner_slug}/{dataset_slug}/versions/{version_number}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    171\u001b[39m     hf_kwargs: Any = \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    172\u001b[39m ) -> Any:  \u001b[38;5;66;03m# noqa: ANN401\u001b[39;00m\n\u001b[32m    173\u001b[39m     warnings.warn(\n\u001b[32m    174\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUse dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    175\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    176\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    177\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zsolt\\OneDrive - Óbudai egyetem\\Deep Learning\\deeplearning\\.venv\\Lib\\site-packages\\kagglehub\\datasets.py:141\u001b[39m, in \u001b[36mdataset_load\u001b[39m\u001b[34m(adapter, handle, path, pandas_kwargs, sql_query, hf_kwargs, polars_frame_type, polars_kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.PANDAS:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkagglehub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpandas_datasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_pandas_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43msql_query\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m adapter \u001b[38;5;129;01mis\u001b[39;00m KaggleDatasetAdapter.POLARS:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkagglehub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpolars_datasets\u001b[39;00m  \u001b[38;5;66;03m# noqa: PLC0415\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zsolt\\OneDrive - Óbudai egyetem\\Deep Learning\\deeplearning\\.venv\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:86\u001b[39m, in \u001b[36mload_pandas_dataset\u001b[39m\u001b[34m(handle, path, pandas_kwargs, sql_query)\u001b[39m\n\u001b[32m     84\u001b[39m pandas_kwargs = {} \u001b[38;5;28;01mif\u001b[39;00m pandas_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pandas_kwargs\n\u001b[32m     85\u001b[39m file_extension = os.path.splitext(path)[\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m read_function = \u001b[43m_validate_read_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_extension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Now that everything has been validated, we can start downloading and processing\u001b[39;00m\n\u001b[32m     89\u001b[39m filepath = dataset_download(handle, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zsolt\\OneDrive - Óbudai egyetem\\Deep Learning\\deeplearning\\.venv\\Lib\\site-packages\\kagglehub\\pandas_datasets.py:108\u001b[39m, in \u001b[36m_validate_read_function\u001b[39m\u001b[34m(file_extension, sql_query)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_extension \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_READ_FUNCTIONS_BY_EXTENSION:\n\u001b[32m    104\u001b[39m     extension_error_message = (\n\u001b[32m    105\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported file extension: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_extension\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSupported file extensions are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(SUPPORTED_READ_FUNCTIONS_BY_EXTENSION.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(extension_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    110\u001b[39m read_function = SUPPORTED_READ_FUNCTIONS_BY_EXTENSION[file_extension]\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read_function \u001b[38;5;129;01mis\u001b[39;00m wrapped_read_sql_query \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sql_query:\n",
      "\u001b[31mValueError\u001b[39m: Unsupported file extension: ''. Supported file extensions are: .csv, .tsv, .json, .jsonl, .xml, .parquet, .feather, .sqlite, .sqlite3, .db, .db3, .s3db, .dl3, .xls, .xlsx, .xlsm, .xlsb, .odf, .ods, .odt"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
